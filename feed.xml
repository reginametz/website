<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://scs.community/feed.xml" rel="self" type="application/atom+xml" /><link href="https://scs.community/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-03-21T11:52:40+00:00</updated><id>https://scs.community/feed.xml</id><title type="html">Sovereign Cloud Stack</title><subtitle>Sovereign Cloud Stack combines the best of cloud computing in one unified standard. SCS is built, backed, and operated by an active open-source community worldwide. Together we put users in control of their data by enabling cloud operators through a decentralized and federated cloud stack – leveraging true digital sovereignty to foster trust in clouds.</subtitle><entry><title type="html">SCS Release 6 published</title><link href="https://scs.community/release/2024/03/20/release6/" rel="alternate" type="text/html" title="SCS Release 6 published" /><published>2024-03-20T00:00:00+00:00</published><updated>2024-03-21T11:48:02+00:00</updated><id>https://scs.community/release/2024/03/20/release6</id><content type="html" xml:base="https://scs.community/release/2024/03/20/release6/"><![CDATA[<h1 id="sovereign-full-stack-cloud-solution">Sovereign full-stack cloud solution</h1>
<h2 id="sovereign-cloud-stack-publishes-release-6">Sovereign Cloud Stack publishes Release 6</h2>

<p><strong>Berlin, 20.03.2024</strong>: The Sovereign Cloud Stack project publishes its sixth release.</p>

<p>Release 6 focuses on the SCS Kubernetes Distribution Cluster Stacks, which can
be rolled out directly on the IaaS reference implementation via the Cluster
Stack Operator. The reference implementation now also includes the
Observability Platform and enhanced Identity and Access Management. The
collaboration with ALASCA e.V. has resulted in enhanced speed and scope in the
development of comprehensive open standards, in line with the requirements of
the German Administrative Cloud Strategy. Overall, SCS Release 6 offers an
easy-to-operate, sovereign full-stack cloud solution.</p>

<p>SCS provides all the cloud technology foundations for realizing digital
sovereignty and implementing open source strategies. Many users of cloud
services from the public sector, but also from the private sector and science,
expect “cloud” to be container-based technology that can ideally be hosted and
operated in a digitally sovereign manner. This is now even easier to achieve:
SCS offers a digitally sovereign, secure, complete, standardized open source
container layer as the basis for all containerized applications.</p>

<h3 id="scs-full-stack-unification-of-iaas-and-kaas-reference-implementation-completed">SCS Full Stack: Unification of IaaS and KaaS reference implementation completed</h3>

<p>Release 6 introduces the Cluster Stacks, the second version of the Kubernetes
as a Service reference implementation from SCS, which can now be seamlessly
rolled out on the Infrastructure as a Service layer from SCS thanks to the
implementation of the Cluster Stack Operator. The cluster stacks are fully
implemented with Kubernetes on-board tools such as operators and thus replace
the old implementation, which still integrated and automated the cluster API
with the help of scripts. The cluster stacks are complemented with identity and
access management and an observability platform which ensure that the stack is
more secure and easier to operate.</p>

<p>SCS thus offers an integrated full-stack cloud solution that combines a
scalable, secure cloud infrastructure and a container platform that can be
developed and tested according to the latest requirements. SCS continues to
offer a fully open standards-based software solution developed openly by many
contributors, which can be influenced and shaped by operators and users and
offers a high degree of transparency - the perfect basis for driving digitally
sovereign innovation at all levels in Germany and Europe.</p>

<h3 id="cooperation-on-standardization">Cooperation on standardization</h3>

<p>In the fall of last year, the cooperation between the SCS project and ALASCA
e.V. was intensified once again. The further development of open standards, as
called for in the German Administrative Cloud Strategy, for example, is at the
forefront of this cooperation. Release 6 shows how fruitful this cooperation
already is, with examples of SCS standards including</p>

<ul>
  <li>New stable version 4 for the SCS-compatible IaaS certification.</li>
  <li>New draft version 1 for the new SCS-compatible KaaS certification.</li>
</ul>

<p>For the operators of SCS cloud environments, it is an advantage that the
implementation of the standards and the resulting certification provides them
with a simple quality criterion (in the best sense of the word) that relates
not only to interoperability and openness, but also to robustness, security and
up-to-dateness. In this way, they not only assure their customers of the
quality of their offering, but also themselves. Of course, interoperability is
of particular interest to users: they can roll out their workloads on the
clouds of multiple operators without any major surprises, which gives them a
high degree of flexibility and in some cases may be a prerequisite to open up
an alternative to the large hyperscalers (AWS, Google, Azure).</p>

<h3 id="simple-operation-of-cloud-environments">Simple operation of cloud environments</h3>

<p>Overall, Release 6 makes it clear that SCS cloud infrastructure can be operated
efficiently: this was and is one of the central value propositions of the SCS
project, alongside control and simple provisioning for end users. The operating
tools have been consistently expanded to simplify everyday work steps for the
operating staff. The continuous standardization of further aspects and
especially the implementation of the standards by providers who do not use the
IaaS reference implementation underline the relevance of the standardization
work. With the latest developments and the release of Cluster Stacks for
OpenStack, deployment environments for Kubernetes clusters are easier and more
efficient to manage.</p>

<p>In the reference environment, a standardized Kubernetes environment is now also
available at the infrastructure level, into which operators can integrate their
own tools (e.g. for billing or capacity management); with the Identity and
Access Management (IAM) from the SCS reference implementation, a supplied
service also makes use of this integration option. The expansion of the SCS
documentation also contributes directly to the efficient operation of SCS cloud
environments; examples, instructions and links to the repositories make the
structure of SCS clear, comprehensible and efficient.</p>

<h3 id="about-the-sovereign-cloud-stack-project">About the Sovereign Cloud Stack Project</h3>

<p>SCS has been funded by the German Federal Ministry of Economics and Climate
Protection (BMWK) since July 2021 and is run by the Open Source Business
Alliance - Bundesverband für digitale Souveränität e.V.. A growing,
international ecosystem of now over 25 companies contributes to the success of
the Sovereign Cloud Stack with over 50 software developers. Together, open
standards for a modern, federatable open source cloud and container platform
are defined and implemented in an open development process using proven open
source components. At the same time, operational knowledge and practice is
being made transparently available to minimize the difficulty of delivering
high-quality and secure cloud services. Already six providers are using SCS
technology productively to operate truly sovereign and GDPR-compliant public
cloud offerings. Additional SCS-based cloud infrastructure (public and private
clouds) is under construction. SCS also contributes to Gaia-X and provides the
development platform for the Gaia-X Federation Services / Cross-Federation
Service Components (GXFS/XFSC).</p>

<h3 id="about-the-open-source-business-alliance-osb-alliance-ev">About the Open Source Business Alliance (OSB Alliance) e.V.</h3>

<p>The Open Source Business Alliance (OSB Alliance) is the association of the open
source industry in Germany. It represents over 200 member companies that
generate more than EUR 126 billion annually in total. Together with scientific
institutions and user organizations, it works to sustainably anchor the central
importance of open source software and open standards for a successful digital
transformation in the public awareness. In addition, innovations in the field
of open source are to be promoted. The goal of the OSB Alliance is to establish
open source as a standard in public procure­ment and in research and business
development. After all, open source and open standards are compelling
foundations for digital sovereignty, innovation capability and security in the
digital transformation and thus the answer to one of the greatest challenges of
our time.</p>

<h3 id="related-links">Related links:</h3>

<p>Sovereign Cloud Stack: https://scs.community/</p>

<p>Technical Documentation SCS: https://docs.scs.community/docs</p>

<p>SCS Repositories: https://github.com/SovereignCloudStack</p>

<p>Release notes: https://github.com/SovereignCloudStack/release-notes</p>

<p>SCS’s notion of digital sovereignty: https://the-report.cloud/why-digital-sovereignty-is-more-than-mere-legal-compliance/ and https://link.springer.com/epdf/10.1007/s11623-022-1669-5?sharing_token=ie7xTVzv_afod07w5Y2lJfe4RwlQNchNByi7wbcMAY4yFyxh9Qw2iCtygUYjun7MI5leBYqiHZBlIeTPv8Sm1Wv8c1dEUf6ebSwnRfo99_nAYh2FgwUyIHjFyZFWv_EIOEIetr2eBSiAPrI68ptBgKxMVkNlS4udZRAhx1X-WB8=</p>

<p>Release Blog Posts: https://scs.community/de/2023/12/27/scs-r6-enables/ und https://scs.community/de/2024/02/01/road-ro-r6/</p>

<p>Contact: OSB Alliance e.V.</p>

<p>Copyright picture: Engin Akyurd</p>]]></content><author><name>[&quot;Sovereign Cloud Stack&quot;]</name></author><category term="release" /><summary type="html"><![CDATA[Sovereign full-stack cloud solution Sovereign Cloud Stack publishes Release 6]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://scs.community/six.jpg" /><media:content medium="image" url="https://scs.community/six.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Sovereign Cloud Stack Security Advisory OVN (CVE-2024-2182)</title><link href="https://scs.community/security/2024/03/15/cve-2024-2182/" rel="alternate" type="text/html" title="Sovereign Cloud Stack Security Advisory OVN (CVE-2024-2182)" /><published>2024-03-15T00:00:00+00:00</published><updated>2024-03-21T11:48:02+00:00</updated><id>https://scs.community/security/2024/03/15/cve-2024-2182</id><content type="html" xml:base="https://scs.community/security/2024/03/15/cve-2024-2182/"><![CDATA[<h2 id="the-vulnerability">The vulnerability</h2>

<p>Multiple versions of OVN (Open Virtual Network) are vulnerable to
crafted BFD packets potentially causing denial of service.</p>

<p>OVN supports configuration of gateway chassis and high-availability
chassis groups (via the Gateway_Chassis and HA_Chassis_Group tables in
the OVN_Northbound database).  These group cluster nodes (chassis)
together and provide high availability to them.  OVN logical switch and
router ports can be configured to reference such groups.  In this case
the traffic forwarding decision is influenced by the liveness of the
chassis listed in the group.</p>

<p>In such scenarios OVN automatically enables the OVS Bidirectional
Forwarding Detection (BFD) functionality to monitor the health of remote
nodes and tunnels between them.</p>

<p>BFD packets are transmitted in-band in tunnels that connect OVN chassis,
along with other traffic.  And, by default, OVS will process any BFD
packets received on a tunnel port with BFD enabled.  That makes it
possible for a VM or container connected to an OVN logical switch port
to send BFD packets that will be tunneled to another node and processed
by OVS, potentially changing the BFD state and affecting the forwarding
decisions.</p>

<p>The vulnerability has been assigned <a href="https://www.cve.org/CVERecord?id=CVE-2024-2182">CVE-2024-2182</a>.</p>

<h2 id="impact-on-the-scs-reference-implementation">Impact on the SCS reference implementation</h2>

<p>The <a href="https://scs.community/">Sovereign Cloud Stack</a> reference
implementation (by <a href="https://osism.tech/">OSISM</a>) comes with and utilizes 
OVN. The upcoming Release 6 of the SCS reference implementation will
not be vulnerable to this as Release 6 will contain the OVN <a href="https://www.ovn.org/en/releases/changelog_v24.03.1/">version 24.03.1</a>.</p>

<p>A way to determine if BFD will be used is to issue the following
commands on the node that runs the OVN central components:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ docker exec -it ovn_nb_db ovn-nbctl --columns name,gateway_chassis list logical_router_port
</code></pre></div></div>

<p>If the above command returns more than a single gateway chassis
reference for a given port that means OVS BFD has been automatically
enabled.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ docker exec -it ovn_nb_db ovn-nbctl --columns name,ha_chassis list ha_chassis_group
</code></pre></div></div>

<p>The same applies if the above command returns groups that contain more
than one chassis.</p>

<p>In any case, operators of OpenStack platforms are advised to deploy fixed versions.</p>

<h2 id="mitigation-and-fixes">Mitigation and Fixes</h2>

<p>For any version of OVN, in order to prevent the issue, an ACL (Access
Control List) rule can be added to drop BFD packets originated from
logical ports.</p>

<p>For example, the following shell script would configure ACLs on all
existing OVN logical switches:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  for sw in $(docker exec -it ovn_nb_db ovn-nbctl --bare --columns name list logical_switch); do
      docker exec -it ovn_nb_db ovn-nbctl acl-add $sw from-lport 32767 'udp &amp;&amp; udp.dst == 3784' drop
  done
</code></pre></div></div>

<p>We do not recommend attempting to mitigate the vulnerability this way
because this will also drop legitimate BFD traffic originated by the
workloads connected to logical switch ports, e.g., BFD sessions
established with external entities.</p>

<p>For the currently maintained version SCS R5 (OSISM 6.0.x), OVN has been updated to <a href="https://www.ovn.org/en/releases/changelog_v23.06.3/">23.06.3</a>.
Updated images have been built by <a href="https://osism.tech/">OSISM</a> and are available to be installed.</p>

<p>To install the fixed OVN images, set <code class="language-plaintext highlighter-rouge">ovn_tag: "2023.1"</code> in the configuration repository in <code class="language-plaintext highlighter-rouge">environments/kolla/configuration.yml</code>.
Afterwards run <code class="language-plaintext highlighter-rouge">osism apply -a upgrade ovn</code>.</p>

<p>This will deploy the latest OVN images for OSISM 6.0.x series (SCS R5, using OpenStack 2023.1 (Antelope)), pulling OVN 23.06.3.
This only replaces the OVN image and no service interruption is to be expected.</p>

<h2 id="version-history">Version history</h2>

<ul>
  <li>Initial Draft, v0.1, 2024-03-14, 23:45 CET.</li>
</ul>]]></content><author><name>[&quot;Felix Kronlage-Dammers&quot;]</name></author><category term="security" /><summary type="html"><![CDATA[The vulnerability]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://scs.community/default-card.jpg" /><media:content medium="image" url="https://scs.community/default-card.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Sovereign Cloud Stack Security Advisory Open vSwitch (CVE-2023-3966)</title><link href="https://scs.community/security/2024/02/20/cve-2023-3966/" rel="alternate" type="text/html" title="Sovereign Cloud Stack Security Advisory Open vSwitch (CVE-2023-3966)" /><published>2024-02-20T00:00:00+00:00</published><updated>2024-03-21T11:48:02+00:00</updated><id>https://scs.community/security/2024/02/20/cve-2023-3966</id><content type="html" xml:base="https://scs.community/security/2024/02/20/cve-2023-3966/"><![CDATA[<h2 id="the-vulnerability">The vulnerability</h2>

<p>Multiple versions of Open vSwitch are vulnerable to crafted Geneve
packets causing invalid memory accesses and potential denial of service.
Triggering the vulnerability requires that Open vSwitch has flow hardware
offload with Linux TC flower enabled (other_config:hw-offload=true).
It is not enabled by default.</p>

<p>The issue is caused by insufficient validation of Geneve metadata
fields in the offload path.  Open vSwitch versions 2.12 and newer are
affected.</p>

<p>The vulnerability has been assigned <a href="https://www.cve.org/CVERecord?id=CVE-2023-3966">CVE-2023-3966</a>.</p>

<h2 id="impact-on-the-scs-reference-implementation">Impact on the SCS reference implementation</h2>

<p>The <a href="https://scs.community/">Sovereign Cloud Stack</a> reference
implementation (by <a href="https://osism.tech/">OSISM</a>) comes with and utilizes 
Open vSwitch. While hardware offloading is not enabled by default most
environments will have it enabled.</p>

<p>The current state of configuration can be checked with this command:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dragon@manager:~$ docker exec -it openvswitch_vswitchd ovs-vsctl get Open_vSwitch . other_config:hw-offload
ovs-vsctl: no key "hw-offload" in Open_vSwitch record "." column other_config
</code></pre></div></div>

<p>If hardware offloading is enabled the output differs:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dragon@manager:~$ docker exec -it openvswitch_vswitchd ovs-vsctl get Open_vSwitch . other_config:hw-offload
"true"
</code></pre></div></div>

<p>This is typically enabled through <code class="language-plaintext highlighter-rouge">openvswitch_hw_offload: "yes"</code> in the host vars or <code class="language-plaintext highlighter-rouge">configuration.yml</code>.</p>

<p>In any case, operators of OpenStack platforms are advised to deploy fixed versions.</p>

<h2 id="mitigation-and-fixes">Mitigation and Fixes</h2>

<p>For any version of Open vSwitch, disabling flow hardware offload will
prevent the issue (setting other_config:hw-offload=false and reboot
the system).  We do not recommend attempting to mitigate the vulnerability
this way because it may impact the overall system throughput.</p>

<p>By default, flow hardware offload support is not enabled.</p>

<p>Open vSwitch has been updated to <a href="https://www.openvswitch.org/releases/NEWS-3.1.4.txt">version 3.1.4</a>.
Updated images have been built by <a href="https://osism.tech/">OSISM</a> and are available to be installed.</p>

<p>Set <code class="language-plaintext highlighter-rouge">openvswitch_tag: "2023.1"</code> in the configuration repository in <code class="language-plaintext highlighter-rouge">environments/kolla/configuration.yml</code>.
Afterwards run <code class="language-plaintext highlighter-rouge">osism apply -a upgrade openvswitch</code>.</p>

<p>This will deploy the latest openvswitch images for OSISM 6.0.x series (SCS R5, using OpenStack Antelope), pulling openvswitch 3.1.4.
This only replaces the Open vSwitch image. Open vSwitch only runs on the data and network plane.
Connections may be interrupted when restarting openvswitch-vswitchd. If this is to be avoided workloads should
be live migrated and the update should by done node by node.</p>

<h2 id="version-history">Version history</h2>

<ul>
  <li>Initial Draft, v0.1, 2024-02-15, 23:45 CET.</li>
  <li>Initial published version, v0.2, 2024-02-20, 12:00 CET.</li>
</ul>]]></content><author><name>[&quot;Felix Kronlage-Dammers&quot;]</name></author><category term="security" /><summary type="html"><![CDATA[The vulnerability]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://scs.community/default-card.jpg" /><media:content medium="image" url="https://scs.community/default-card.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">SDN scalability improvements</title><link href="https://scs.community/2024/02/09/sdn-scalability/" rel="alternate" type="text/html" title="SDN scalability improvements" /><published>2024-02-09T00:00:00+00:00</published><updated>2024-03-21T11:48:02+00:00</updated><id>https://scs.community/2024/02/09/sdn-scalability</id><content type="html" xml:base="https://scs.community/2024/02/09/sdn-scalability/"><![CDATA[<h1 id="overview-of-sdn-network-scalability">Overview of SDN network scalability</h1>

<p>Software-defined Networking (SDN) is a critical component of modern cloud environments, providing the necessary flexibility and scalability to meet the demands of large-scale network infrastructures. As the size and complexity of cloud deployments continue to grow, the need for scalable and efficient SDN solutions becomes increasingly important. In this post, we explore the challenges and strategies for improving SDN scalability within Sovereign Cloud Stack (SCS) and OpenStack environments.</p>

<h1 id="the-data-center-network">The data center network</h1>

<figure class="figure mx-auto d-block" style="width:50%">
  <a href="/assets/images/blog/dc-network-architectures-9ce41bf73ef654366c35b856f83c46b7919b13e55e65819bd9a832cbb70629b6f4bf5dd57a77ba9e8bbfdeca4a70a8a06f8a885939f7d9061fed7e9d9472d730.png">
    <img class="figure-img w-100" integrity="sha512-nOQb9z72VDZsNbhW+DxGt5GbE+VeZYGb2agyy7cGKbb0v13Vene6nou/3spKcKigb4qIWTn32QYf7X6dlHLXMA==" crossorigin="anonymous" src="/assets/images/blog/dc-network-architectures-9ce41bf73ef654366c35b856f83c46b7919b13e55e65819bd9a832cbb70629b6f4bf5dd57a77ba9e8bbfdeca4a70a8a06f8a885939f7d9061fed7e9d9472d730.png" />
  </a>
</figure>

<p>A modern data center network typically uses a 2-tier spine-leaf network architecture. More traditional 3-tier architectures are becoming less popular.</p>

<p>In a spine-leaf network, all servers are connected to one or two TOR (top of rack) or Leaf switches within a rack. All leaf switches are connected to all spine switches in the layer above. There are no direct connections between leaf or spine switches on the same tier. This kind of network facilitates scalability by being simple and easy to support.</p>

<p>On top of this physical network, also called underlay, all Software Defined Networking (SDN) features are built on top of. The SDN is the virtual network managed by end users and connects VMs, containers, and sometimes physical servers. The SDN is entirely virtualized and must be designed and implemented with the physical topology in mind to achieve the desired performance, functionality, and scalability requirements.</p>

<h1 id="overview-of-sdn-in-openstack">Overview of SDN in OpenStack</h1>

<p>Software-defined Networking (SDN) is a networking concept where software-based controllers and APIs direct network traffic rather than the networking hardware. In OpenStack, SDN networks are used to connect virtual machines and containers. The physical network is also referred to as the underlay, while SDN is called overlay. Larger operators and an increased number of containerized workloads put stress on the SDN stack.</p>

<p>OpenStack Neutron is the main component responsible for networking. It heavily leverages open-source technologies like OVN/OVS and communicates with the underlay network to provide higher-level network services to users. ML2 is the Neutron plugin responsible for communication with the physical network and can have multiple drivers each driver supporting particular hardware/vendor devices. The SDN stack is distributed where some components run on dedicated network nodes, network devices, and even servers.</p>

<p>When OpenStack traffic increases the network must scale accordingly. There are several aspects of SDN scalability and they depend on the actual network architecture. We dive deeper below, but some important points to consider are SDN bottlenecks (OVN control plane, network node resources), support for several tenants and networks, and actual data-plane performance. In this post, we will explore those challenges and strategies to improve SDN scalability in OpenStack and SCS.</p>

<h2 id="sdn-stack">SDN stack</h2>

<p>In this section, we will take a look at more software components needed to implement SDN in OpenStack.</p>

<h3 id="neutron">Neutron</h3>

<p>Neutron is an OpenStack project to provide “network connectivity as a service” between interface devices (e.g., vNICs) managed by other OpenStack services (e.g., nova). It implements the OpenStack Networking API. Neutron is responsible for the centralized management of tenant networks, devices, and address allocation. It orchestrates the physical network, and the devices by using plugins and drivers for each particular technology chosen in the stack.</p>

<h3 id="ovnovs">OVN/OVS</h3>

<p>Open Virtual Network (OVN) and Open vSwitch (OVS) are integral components of Neutron’s network architecture. OVN provides network virtualization, offering logical network abstractions such as logical switches and routers. It is tightly coupled with OVS, which is a high-performance, multilayer virtual switch. Together, OVN/OVS enable dynamic, programmable network setups, allowing for efficient routing, switching, and bridging of traffic in virtualized environments. Their integration with Neutron facilitates complex networking scenarios, ensuring scalability and flexibility in handling network traffic within OpenStack deployments.</p>

<h3 id="bgpevpn">BGP/EVPN</h3>

<p>Border Gateway Protocol (BGP) and Ethernet Virtual Private Network (EVPN) are technologies used for advanced networking functionalities. BGP is a standardized exterior gateway protocol designed to exchange routing and reachability information among autonomous systems on the internet. EVPN extends BGP to support scalable, multi-tenant layer 2 VPN services. In SCS, BGP/EVPN is used for efficient routing and segmentation in large-scale cloud environments. It enables more robust and flexible networking solutions by allowing dynamic route advertisement, enhanced traffic engineering, and seamless integration with existing network infrastructures.</p>

<h3 id="tunneling-protocols---vxlangeneve">Tunneling protocols - VXLAN/Geneve</h3>

<p>Virtual Extensible LAN (VXLAN) and Generic Network Virtualization Encapsulation (Geneve) are network overlay protocols used in OpenStack and OVN/OVS for tunneling network traffic over existing network infrastructures. VXLAN is widely used for encapsulating Ethernet frames over a UDP tunnel, enabling layer 2 networks to extend over layer 3 networks. Geneve, a newer protocol, offers similar capabilities but with additional flexibility and extensibility. In SCS, VXLAN/Geneve is critical for creating isolated, multi-tenant networks over a shared physical network infrastructure. This encapsulation allows scalable network segmentation, providing secure and efficient communication channels within cloud environments.</p>

<h2 id="physical-network">Physical network</h2>

<p>The physical network layer is crucial as it provides the foundational infrastructure on which all virtualized network functions are built upon. This layer typically involves various network hardware vendors who supply the physical networking equipment such as switches, routers, and other hardware. These devices are essential for data transmission and routing in a physical setup. Network Operating Systems (NOS) are also a component of the physical network layer. NOS is the software running on network devices, enabling them to perform networking functions and interact with higher-level SDN controllers and applications. In OpenStack Neutron, the ML2 (Modular Layer 2) plugin plays a key role in the physical network layer. ML2 provides an abstraction layer that supports multiple networking mechanisms in a pluggable manner, allowing Neutron to interface with various types of networking hardware and technologies. This plugin architecture ensures that Neutron can work with a wide range of physical networking equipment and configurations, thus enabling flexibility and scalability in the physical network infrastructure of an OpenStack-based SDN deployment.</p>

<h2 id="bare-metal-hosts">Bare metal hosts</h2>

<p>Bare metal hosts in the context of OpenStack and SDN play a significant role in providing high-performance, non-virtualized computing resources. These hosts are typically equipped with advanced network interface cards (NICs), such as SmartNICs and Data Processing Units (DPUs). SmartNICs are intelligent network cards that offload processing tasks that would typically be handled by the server CPU. These include functions like traffic shaping, encryption/decryption, and network packet processing. The use of SmartNICs can lead to improved performance and reduced CPU load on the bare metal hosts.</p>

<p>DPUs are an evolution of SmartNICs and are also very common in clouds providing physical servers to users. The difference between SmartNICs is that DPUs usually run their own OS and can be managed independently of the host OS. They are more like computers within a computer. Bare metal servers are directly accessed by the users, which creates a constraint on the SDN network because it should not collide with user workloads. Offloading SDN to the DPU is the best solution for such use cases.</p>

<p>Incorporating SmartNICs and DPUs into bare metal hosts within an OpenStack environment enhances the network’s flexibility, efficiency, and performance. This setup is particularly beneficial for workloads that require high throughput, low latency, and secure network communications. As such, bare metal hosts equipped with these advanced NICs are an essential component of modern SDN architectures, particularly in environments where performance and security are critical.</p>

<h1 id="common-network-designs">Common network designs</h1>

<p>Below we dive deeper into several common network designs used for SCS/OpenStack deployments.</p>

<h2 id="vlans">VLANs</h2>

<figure class="figure mx-auto d-block" style="width:50%">
  <a href="/assets/images/blog/VLANs_network_design-9430b0fb97b59353f0da0437cbba988c5299541ce868b42302c5efaadd71c50843b5ec7eee40eb7ac1756baa1820eb32945a5e5e2757167a66cd87f6d33532f2.png">
    <img class="figure-img w-100" integrity="sha512-lDCw+5e1k1Pw2gQ3y7qYjFKZVBzoaLQjAsXvqt1xxQhDtex+7kDresF1a6oYIOsylFpeXidXFnpmzYf20zUy8g==" crossorigin="anonymous" src="/assets/images/blog/VLANs_network_design-9430b0fb97b59353f0da0437cbba988c5299541ce868b42302c5efaadd71c50843b5ec7eee40eb7ac1756baa1820eb32945a5e5e2757167a66cd87f6d33532f2.png" />
  </a>
</figure>

<p>VLANs are the simplest and most popular design for small operators that are just starting to use OpenStack.</p>

<p>This type of network leverages VLANs between network switches and servers. Each tenant network is assigned its own VLAN. User workloads like VMs and containers on the server are bound to a VLAN by OVS. Neutron orchestrates the process of creation and management of tenant networks both on the network plane (underlay network) and on the servers. It uses ML2 to provision VLANs on the network switches and delegates to OVN/OVS the binding of the virtual device to the VLAN on the server.</p>

<p>Operators have the option to either pre-provision all VLANs ahead of time so that when a new tenant network appears it can be attached to an existing VLAN without needing to talk to the network devices again. The alternative is to use ML2 to dynamically configure new VLAN before each tenant network is created.</p>

<h3 id="pros">Pros</h3>

<ul>
  <li>This kind of network is easy to implement. It can be simplified even further by pre-provisioning all VLANs on the physical network thus eliminating the need for ML2. It is a good approach for test beds and small deployments.</li>
</ul>

<h3 id="cons">Cons</h3>

<ul>
  <li>Most notably, VLANs don’t scale well. This is due to a theoretical limit of 4096 VLANs on each network switch. A more realistic limit would be about 100 OpenStack tenants for a single cloud network. This is a more practical limit based on real world experience.</li>
  <li>The approach is also very fragile. A single VM can break the whole network, thus exposing a huge blast radius. Also, network switches are notorious for being able to persist VLAN configuration. If a switch goes down, its configuration must be replayed after boot-up which is a process leading to significant downtimes.</li>
  <li>Another drawback, which we see is the involvement of the physical network to support the SDN. For each new tenant network in OpenStack, new VLANs must be provisioned. Even in the pre-provisioned scenario, the VLANs must still exist in the underlay, which eventually becomes a bottleneck.</li>
</ul>

<h2 id="network-centric-sdn">Network-centric SDN</h2>

<figure class="figure mx-auto d-block" style="width:50%">
  <a href="/assets/images/blog/Network_centric_network_design-c715b0fbbb492816b154013ff2f11334e494c5953b8a542c3fa8ee621f0f4325222e6bc52b886afea45a50b1a56d41ed5dbb41a4c9c44cf1e79dc797f8f8db0f.png">
    <img class="figure-img w-100" integrity="sha512-xxWw+7tJKBaxVAE/8vETNOSUxZU7ilQsP6juYh8PQyUiLmvFK4hq/qRaULGlbUHtXbtBpMnETPHnnceX+PjbDw==" crossorigin="anonymous" src="/assets/images/blog/Network_centric_network_design-c715b0fbbb492816b154013ff2f11334e494c5953b8a542c3fa8ee621f0f4325222e6bc52b886afea45a50b1a56d41ed5dbb41a4c9c44cf1e79dc797f8f8db0f.png" />
  </a>
</figure>

<p>This network design employs a more network-centric approach by implementing most SDN functionalities within the physical network. The key point is that some kind of tunneling protocols like VXLAN or Geneve are used to encapsulate network packets and transport them over the network. In a spine-leaf topology, the leaf or ToR will terminate VXLAN endpoints. For the control plane, BGP (EVPN) is used to transmit layer 2 addressing information between switches.</p>

<p>On the server side, a regular VLAN is provisioned from the ToR switch for each tenant network. Since not all tenant networks are needed on each ToR switch, this design scales better than pure VLANs. Also, no VLANs are used between the switches if the underlay is layer 3.</p>

<h3 id="pros-1">Pros</h3>

<ul>
  <li>This kind of network is more scalable, easily handling up to 1000 OpenStack tenants. It is also more stable and resilient to change, because of the smaller blast radius.</li>
  <li>Since all SDN is offloaded to the physical equipment, server CPUs are not doing any encapsulation/decapsulation and are free to serve the users. Heavy data plane processing is done on dedicated network hardware, which has been optimized for this task. Also, no SmartNICs or DPUs are needed on the servers, which is more cost efficient.</li>
</ul>

<h3 id="cons-1">Cons</h3>

<ul>
  <li>Since this is a network-centric approach, the network is still very much involved in the SDN. Physical device limits shown before like persisting  switch configuration still apply.</li>
  <li>This approach is also more complex than pure VLANs because it requires the network to run routing protocols like BGP and BFD. Neutron still needs to talk the switches via ML2.</li>
  <li>Importantly, traffic between tenant networks (east/west and north/south) is routed via the network nodes, which can become a choke point. So while being better for scalability, this design still has limitations for very large deployments.</li>
</ul>

<h2 id="server-centric-sdn">Server-centric SDN</h2>

<figure class="figure mx-auto d-block" style="width:50%">
  <a href="/assets/images/blog/Server_centric_network_design-96746e887fb08b7561b28fb4c02704408bd2ff27c8c9c3f45962651d9ebe77cebd89fe4c3d93cfce6395b09bdc693a4507c077bb1cc14f2fa0505ded0ab5354f.png">
    <img class="figure-img w-100" integrity="sha512-lnRuiH+wi3Vhso+0wCcEQIvS/yfIycP0WWJlHZ6+d869if5MPZPPzmOVsJvcaTpFB8B3uxzBTy+gUF3tCrU1Tw==" crossorigin="anonymous" src="/assets/images/blog/Server_centric_network_design-96746e887fb08b7561b28fb4c02704408bd2ff27c8c9c3f45962651d9ebe77cebd89fe4c3d93cfce6395b09bdc693a4507c077bb1cc14f2fa0505ded0ab5354f.png" />
  </a>
</figure>

<p>With the server-centric network design, SDN software features are implemented on the servers and not on the network equipment. Tunneling protocols like VXLAN have terminating endpoints on the servers and the addresses for those endpoints are discovered via control plane protocols like BGP/EVPN. Also, encapsulation/decapsulation and crypto processing are done on the server.</p>

<p>This design reduces reliance on the physical network to carry out SDN-related tasks. The only requirement for the underlay is that it must be Layer 3 and run BGP to enable server-centric SDN.</p>

<p>Because of the limited involvement of the physcal network, this design scales very well. The downside, however, is that datapath processing is performed on the server. An example of such processing is VXLAN encapsulation/decapsulation. If no DPU or SmartNIC exists on the server, this logic runs on the CPU, which can negatively impact its performance. It is recommended to deploy servers running SmartNICs to help with data-plane performance.</p>

<h3 id="pros-2">Pros</h3>

<ul>
  <li>very scalable</li>
  <li>very efficient datapath routing (ingress/egress)</li>
  <li>simple network underlay</li>
  <li>no network orchestration in OpenStack</li>
</ul>

<h3 id="cons-2">Cons</h3>

<ul>
  <li>complex server</li>
  <li>might require SmartNICs and DPUs to offload server CPU</li>
  <li>must run routing protocols in the server</li>
</ul>

<h2 id="hybrid---sdn-on-both-servers-and-switches">Hybrid - SDN on both servers and switches</h2>

<figure class="figure mx-auto d-block" style="width:50%">
  <a href="/assets/images/blog/Hybrid_network_design-d5e1db2780c792238b93fef53249f32e3b8b2627e27c3299c96cdce815645da53c86d2b3cf122c142339079e293190a0213e971b2b95b146a3e330af5fc8229f.png">
    <img class="figure-img w-100" integrity="sha512-1eHbJ4DHkiOLk/71MknzLjuLJififDKZyWzc6BVkXaU8htKzzxIsFCM5B54pMZCgIT6XGyuVsUaj4zCvX8ginw==" crossorigin="anonymous" src="/assets/images/blog/Hybrid_network_design-d5e1db2780c792238b93fef53249f32e3b8b2627e27c3299c96cdce815645da53c86d2b3cf122c142339079e293190a0213e971b2b95b146a3e330af5fc8229f.png" />
  </a>
</figure>

<p>A variation of the server-centric design, the hybrid network supports bare metal nodes, which are allocated to single users. Since the user has full access to the OS of the server, the SDN functionality cannot be deployed alongside applications, since SDN is managed by the service provider. We have two valid options for deploying SDN components in such network:</p>

<ul>
  <li>dedicated customer switch, attached to a single bare metal node</li>
  <li>a DPU card attached to the bare metal node</li>
</ul>

<p>SmartNIC is not a good solution, because it is more tightly coupled to the host node and exposed to the end user. DPUs and switches can be  independently managed by the provider. This separation helps avoid collision between network infrastructure and user workloads.</p>

<h1 id="solutions-for-better-sdn-scalability">Solutions for better SDN scalability</h1>

<p>In our pursuit to enhance the scalability and performance of SDN networking within Sovereign Cloud Stack (SCS), we have explored several approaches. We focus on leveraging cutting-edge network designs and technologies such as SONiC to meet the growing scalability demands of modern network infrastructures.</p>

<h2 id="network-design---vxlan-on-servers">Network design - VXLAN on Servers</h2>

<p>For scenarios that utilize a “VXLAN on Servers” network design, we emphasize the use of SmartNICs and Data Processing Units (DPUs) to significantly improve the data plane performance characteristics of the network.</p>

<h3 id="smartnics-and-dpus">SmartNICs and DPUs</h3>

<figure class="figure mx-auto d-block" style="width:50%">
  <a href="/assets/images/blog/nvidia-bluefield-3-dpu-3c33-d-988a0e4115231e7a53f7f4407009c976ac1f5b7d9d280e07dcd5114193b847329aa94e5fb6639adb8278da09c41d17e6854cbb2e0479df2be17806f6bb2df0ef.jpg">
    <img class="figure-img w-100" integrity="sha512-mIoOQRUjHnpT9/RAcAnJdqwfW32dKA4H3NURQZO4RzKaqU5ftmOa24J42gnEHRfmhUy7LgR53yvheAb2uy3w7w==" crossorigin="anonymous" src="/assets/images/blog/nvidia-bluefield-3-dpu-3c33-d-988a0e4115231e7a53f7f4407009c976ac1f5b7d9d280e07dcd5114193b847329aa94e5fb6639adb8278da09c41d17e6854cbb2e0479df2be17806f6bb2df0ef.jpg" />
  </a>
</figure>

<p>SmartNICs and DPUs play a crucial role in boosting packet processing speeds within the network. SmartNICs are particularly advantageous for virtual machines (VMs), offering tunneling encapsulation/decapsulation and encryption features essential for SDN protocols. By offloading routing and SDN protocol tasks to the server, SmartNICs enhance CPU performance and offer cost-saving benefits.</p>

<p>On the other hand, DPUs are ideally suited for environments where bare metal servers are directly accessed by end-users. Representing the next evolutionary step beyond SmartNICs, DPUs offer the distinct advantage of being managed independently by the network operator, separate from user control. Within SCS, we provide the essential infrastructure and configuration capabilities, enabling users to effortlessly set up and manage SmartNIC and DPU features on their networks.</p>

<h2 id="network-design---vxlan-on-switches">Network design - VXLAN on Switches</h2>

<p>When SDN protocols are implemented directly on network switches, our approach involves exploring the use of SONiC alongside enhancements in tooling and automation to scale effectively.</p>

<h3 id="sonic">SONiC</h3>

<p>Our strategy includes bolstering support for SONiC, a powerful open-source network operating system that enables network scalability and flexibility.</p>

<h3 id="tooling---automating-sonic-rollout-and-configuration">Tooling - Automating SONIC rollout and configuration</h3>

<p>We are committed to enhancing tooling, configuration, and documentation to facilitate the adoption of this network architecture in SCS. This includes improvements in OSISM and Kolla Ansible, along with the integration of Netbox. Netbox serves as the source of truth, allowing for the generation of initial roll-out configurations for OSISM and Kolla Ansible. Additionally, we aim to integrate observability and monitoring for SONiC-based network equipment into SCS, leveraging SONiC’s existing observability features.</p>

<h2 id="physical-network-1">Physical network</h2>

<p>Our efforts extend to improving the dynamic configuration of the physical network from OpenStack. This involves exploring the existing ML2/network-generic-switch driver and expanding its capabilities to support VXLAN and Geneve. We are also examining other drivers/plugins and the potential for developing a new SCS plugin for Neutron. Enhancements related to SCS in SONiC can be packaged independently and installed atop any commercial or community version of SONiC.</p>

<h2 id="sdn-stack-1">SDN stack</h2>

<p>A notable challenge within the OpenStack network nodes is the bottleneck caused by the OVN (Open Virtual Network) control plane software. A potential solution is to migrate the OVN control plane to SONiC. It would allow the OVN control plane to operate in a distributed fashion, leveraging multiple existing SONiC devices within the network for enhanced performance. However, a potential drawback is the resource intensity of OVN, which could strain the limited resources on SONiC devices. Our approach includes carefully evaluating these considerations to ensure a balanced and efficient SDN stack implementation.</p>]]></content><author><name>[&quot;Angel Kafazov&quot;]</name></author><summary type="html"><![CDATA[Overview of SDN network scalability]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://scs.community/default-card.jpg" /><media:content medium="image" url="https://scs.community/default-card.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">SCS R6 release candidates - plan your testing!</title><link href="https://scs.community/2024/02/01/road-ro-r6/" rel="alternate" type="text/html" title="SCS R6 release candidates - plan your testing!" /><published>2024-02-01T00:00:00+00:00</published><updated>2024-03-21T11:48:02+00:00</updated><id>https://scs.community/2024/02/01/road-ro-r6</id><content type="html" xml:base="https://scs.community/2024/02/01/road-ro-r6/"><![CDATA[<h2 id="roadmap-to-scs-release-6">Roadmap to SCS release 6</h2>

<p>The <a href="https://scs.community/">Sovereign Cloud Stack</a> project is developed continuously.
Continuous testing ensures that the latest code remains usable most of the time.
To better serve our cloud provider partners that have limited appetite for change
and are not very tolerant against occasional breakage, we publish releases.</p>

<p>Every 6 months, there is a new major release – this is the time where major
improvements get integrated and where we move to the latest proven versions of the
many upstream projects we work on and contribute to. This is also where we occasionally
have to introduce breaking changes – obviously something that we don’t do easily and
that we carefully communicate by deprecation announcements well ahead of time.</p>

<p>In between those major releases we publish several minor releases that bring incremental
improvements as well as bug and security fixes.</p>

<h2 id="sidenote-scs-standards-releases">Sidenote: SCS Standards releases</h2>

<p>The technologies used in SCS allow for very flexible configuration. This is great!
On the flip side, platforms composed of a very similar set of technologies can look
very different to users and applications, resulting in extra effort to get accustomed
to each platform and sometimes significant effort to automate workloads, especially
if these want to achieve high availability. The SCS project aims to provide a high
degree of portability between SCS-compatible platforms by defining standards.
These are built on top of existing upstream standards and define further aspects
of platform behavior that application developers and operators need.</p>

<p>The SCS software release does obviously implement all the SCS Standards by default;
however the standards can also be fulfilled by independent implementations. The release
cycle of standards is not tightly coupled with the software release. This would
only be needed if a new implementation is required to meet new standards. While
this can happen, it is an exception.</p>

<p>The SCS project is currently finalizing a new set of standard scopes: The version 4
of our <a href="https://docs.scs.community/standards">SCS-compatible standards</a> on the IaaS
layer and the first version of SCS standards
on the Container (Kubernetes) layer. We encourage software developers, DevOps Teams
and Providers alike to look at the
<a href="https://github.com/SovereignCloudStack/standards/">ongoing work</a>, provide
feedback and contribute to the
<a href="https://docs.scs.community/standards/scs-0001-v1-sovereign-cloud-standards">standardization process</a>.</p>

<p>With this out of the way, let’s turn back to our upcoming release.</p>

<h2 id="release-candidates">Release Candidates</h2>

<p>While our CI pipelines do nightly verification of the built software, we
encourage our cloud service provider partners to do additional testing, especially
ahead of a release. There are always issues that are specific to the setups and
configurations of our partners that can not always be found in our nightly pipelines.
In the end, upgrading test and staging platforms ahead of a release also provides
our partners with the experience and confidence in the release and the upgrade process,
so we can not only produce high quality releases, but also have our partners upgrade their
production platforms quickly.</p>

<p>After discussion with our partners, for R6, we have determined to create a more
formal release candidate cadence and a more formalized test results collection
than we used to, reflecting the increasing size of our provider ecosystem as well
as the increasing size of our larger partners’ environments.</p>

<p>Extending our <a href="https://scs.community/tech/2023/11/22/scs-release6-upcoming/">previous announcement</a>
of the R6 release, we have come up with the following schedule for release candidates:</p>

<table>
  <thead>
    <tr>
      <th>R6 RC</th>
      <th>ETA</th>
      <th>notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>RC0</td>
      <td>2024-02-07</td>
      <td>aka 7.0.0a</td>
    </tr>
    <tr>
      <td>RC1</td>
      <td>2024-02-21</td>
      <td> </td>
    </tr>
    <tr>
      <td>RC2</td>
      <td>2024-03-06</td>
      <td> </td>
    </tr>
    <tr>
      <td>RC3</td>
      <td>2024-03-13</td>
      <td>may not be needed and be skipped</td>
    </tr>
    <tr>
      <td>RCx</td>
      <td>2024-03-1y</td>
      <td>if needed in order to retest</td>
    </tr>
    <tr>
      <td>final</td>
      <td>2024-03-20</td>
      <td>aka 7.0.0</td>
    </tr>
  </tbody>
</table>

<p>Note that with the first release candidate (RC0), the large features have
all been completed, been integrated and must be working. We still
work on improving documentation, increasing test coverage and of course
on fixing bugs.
We may grant a small number of exceptions to this rule for so-called late
features to be complete after RC0 (typically with RC1) and we will
make these transparent to our testers to avoid wasting effort.</p>

<h2 id="late-features-high-level-view">Late features (high level view)</h2>

<p>The focus for RC0 is to test the upgrade on the IaaS layer, where OSISM has
integrated the new OpenStack version 2023.2 (aka Bobcat). We have some
work pending in the IAM area and on the container layer where we have the
exciting new technology with Cluster Stacks. Both can be tested already,
but will receive a few more changes until RC1. One of the areas of R6 where
we may not achieve 100% completion is to cover all cases where Kubernetes
Clusters are migrated from the old cluster-API based K8s-as-a-Service
solution (KaaS v1) to the new cluster stacks (also built on top of cluster-API).
Expect improvements here until later release candidates and
read the release notes to understand which scenarios are covered and which
ones may not be covered by full automation yet when we finalize R6.</p>

<p>The high level goals for R6 are documented in the
<a href="https://scs.community/2023/12/29/scs-r6-enables/">R6 outcomes</a> blog article.
A more fine-grained list of features is collected in the R6 release notes that
will get more and more complete over the next weeks.</p>

<h2 id="matrix">Matrix</h2>

<p>The development teams are very focused to respond quickly to issues reported
against our release candidates. We’d like to ask testers to open issues in
github against the relevant repositories as usual. To facilitate quick and
more interactive communication, we also invite testers and developers to
join the <a href="https://matrix.to/#/#scs-r6-releng:matrix.org">SCS | Release Engineering and Testing R6 Matrix room</a> to stay atop of
announcements, reports and responses that come in.</p>]]></content><author><name>[&quot;Kurt Garloff&quot;]</name></author><summary type="html"><![CDATA[Roadmap to SCS release 6]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://scs.community/blog/roadmap-r6.png" /><media:content medium="image" url="https://scs.community/blog/roadmap-r6.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Opensource - Testbed adopts OpenTofu</title><link href="https://scs.community/2024/01/12/testbed-with-opentofu/" rel="alternate" type="text/html" title="Opensource - Testbed adopts OpenTofu" /><published>2024-01-12T00:00:00+00:00</published><updated>2024-03-21T11:48:02+00:00</updated><id>https://scs.community/2024/01/12/testbed-with-opentofu</id><content type="html" xml:base="https://scs.community/2024/01/12/testbed-with-opentofu/"><![CDATA[<p><a href="https://osism.github.io/">OSISM</a> is known as the reference implementation for the Infrastructure-as-a-Service layer in the Sovereign Cloud Stack (SCS) project.
The <a href="https://github.com/osism/testbed">OSISM testbed</a> is therefore used in the SCS project to demonstrate, test and further develop the Infrastructure-as-a-Service layer.</p>

<p>The development of OSISM and SCS presents a number of challenges due to the distributed nature of the system architecture,
which can extend to different levels. In addition to the purely functional testing of infrastructure components, as is possible with
<a href="https://docs.scs.community/docs/iaas/guides/deploy-guide/examples/cloud-in-a-box">Cloud in a box</a> (each component exists exactly once),
it is therefore also very useful to test the various components in a scenario that is more similar to a productive setup by operating them in a cluster setup.
operated in a cluster mode. In this way, some <a href="https://en.wikipedia.org/wiki/Non-functional_requirement">non-functional</a> configuration and implementation issues can also be
implementation issues can be simulated, developed and verified in the testbed.</p>

<p>Virtual machines, network and storage based on OpenStack are used as the basis or to provide the testbed.
The testbed can thus basically operate an SCS system for testing and development purposes in any Openstack Cloud environment.
(As SCS also virtualizes systems itself, the OpenStack environment must provide the capabilities for nested virtualization.)</p>

<p>The SCS testbed previously used Terraform to automatically set up and manage the basis for the required infrastructure components.
Terraform was previously published under the Mozilla Public License v2.0 and was therefore a good fit with its terms of use and openness
to the OSISM and SCS project.</p>

<p>With the <a href="https://www.hashicorp.com/blog/hashicorp-adopts-business-source-license">announcement</a> of August 10, 2023, Hashicorp has announced that this will change in the future - future
versions will be made available under the Business Source License v1.1, at least in relevant parts.
As we have a very strong interest in ensuring that our project is <a href="https://github.com/SovereignCloudStack/standards/blob/main/Drafts/OSS-Health.md">based on serious open source components</a>,
it was foreseeable that we would have to find a sensible alternative that meets our requirements.</p>

<p>Fortunately, on September 2, 2023, a Terraform fork was founded under the umbrella of the Linux Foundation under the
of the Linux Foundation under the terms of the Mozilla Public License 2.
With the first official and stable release of <a href="https://opentofu.org/">OpenTofu</a>, OSISM and SCS are now presenting their Infrastructure-as-Code
realization in the testbed to OpenTofu version 1.6.0.</p>

<p>The migration was very simple: From our point of view, OpenTofu can be described in good conscience and unsurprisingly as a drop-in replacement for Terraform.</p>

<p>With today’s integrated code version, we have made some detailed improvements to the installation of the
dependencies and the <a href="https://docs.osism.tech/testbed/">documentation</a> of the testbed.
It is now only necessary to install <code class="language-plaintext highlighter-rouge">make</code>, <code class="language-plaintext highlighter-rouge">wireguard</code> and <code class="language-plaintext highlighter-rouge">python-virtualenv</code> on the testbed user’s computer.
All other dependencies such as OpenTofu and Ansible are now installed or updated according to the status of the Git-branch.
In this way, we ensure that testbed users will have less effort in the future when managing the tools and
ensure that the testbed is used with the right tools in the right versions.</p>

<p>I would like to take this opportunity to once again express my enthusiasm for OpenSource in connection with Terraform.
The openness of Terraform to date and the commitment of the community have ensured that we can migrate to an alternative and future-proof product with little effort.</p>]]></content><author><name>[&quot;Marc Schöchlin&quot;]</name></author><summary type="html"><![CDATA[OSISM is known as the reference implementation for the Infrastructure-as-a-Service layer in the Sovereign Cloud Stack (SCS) project. The OSISM testbed is therefore used in the SCS project to demonstrate, test and further develop the Infrastructure-as-a-Service layer.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://scs.community/default-card.jpg" /><media:content medium="image" url="https://scs.community/default-card.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Delving into the Technical Depths of Intel-SA-00950 and AMD Cachewarp Vulnerabilities</title><link href="https://scs.community/2024/01/03/intel-amd-cpu-vulns/" rel="alternate" type="text/html" title="Delving into the Technical Depths of Intel-SA-00950 and AMD Cachewarp Vulnerabilities" /><published>2024-01-03T00:00:00+00:00</published><updated>2024-03-21T11:48:02+00:00</updated><id>https://scs.community/2024/01/03/intel-amd-cpu-vulns</id><content type="html" xml:base="https://scs.community/2024/01/03/intel-amd-cpu-vulns/"><![CDATA[<h2 id="the-evolving-landscape-of-security-vulnerabilities">The Evolving Landscape of Security Vulnerabilities</h2>

<p>In the ever-evolving realm of cybersecurity, new vulnerabilities are constantly being discovered and exploited by malicious actors. These vulnerabilities can exploit various aspects of computing systems, ranging from hardware architecture to software implementations. Past August, we already talked about <a href="https://scs.community/2023/08/29/new-cpu-leaks/">CPU leaks</a> which posed a risk for providers and users of SCS clouds.</p>

<p>Recently, two critical vulnerabilities, namely Intel-SA-00950 (reptar) and AMD CacheWarp, have emerged, posing significant threats to system security and data integrity.</p>

<h2 id="intel-sa-00950-unveiling-the-underlying-mechanism">Intel-SA-00950: Unveiling the underlying mechanism</h2>

<p>At the heart of Intel-SA-00950, tracked as CVE-2023-23583, lies a a flaw in the implementation of the fast repeat MOVSB instruction (<em>rep movsb</em>), also known as <strong>FRMS</strong>, in Intel’s IPUs (Infrastructure Processing Units). FRMS is a specialized instruction designed to efficiently copy data between x86 memory locations. The vulnerability lies in the way intel handles certain instruction sequences that contain a mix of instruction prefixes, FRMS and other instructions.</p>

<h3 id="the-root-cause-frms-handling-of-branching-instructions">The Root Cause: FRMS Handling of Branching Instructions</h3>

<p>The vulnerability lies in the interaction between FRMS and conditional (branching) instructions. When a sequence of instructions comprising FRMS and branching instructions is executed, FRMS may misinterpret the branching instructions, leading to unintended activation of IPU functionality.</p>

<p>This unintended activation, in turn, triggers a cascade of events that can  manifest in various ways, including:</p>

<ul>
  <li>
    <p><strong>OOB Memory Accesses</strong>: FRMS, under the influence of the misinterpreted branching instructions, may access memory locations that are not directly accessible to the executing code. This can lead to the disclosure of sensitive data from privileged memory regions.</p>
  </li>
  <li>
    <p><strong>Data Corruption</strong>: The unintended activation of IPU functionality can disrupt the normal operation of the processor, leading to data corruption and unpredictable behavior.</p>
  </li>
  <li>
    <p><strong>System Malfunction</strong>: In extreme cases, the corruption of critical data or the disruption of IPU functionality can cause system crashes or other forms of malfunction, effectively denying service to legitimate users. The system failure is not limited to a virtual machine, but can cause the complete host to go down.</p>
  </li>
</ul>

<p>Our team performed several local tests on systems with affected processors, observing system crashes for all screnarios, even with the attacks being launched from virtual machines.</p>

<h3 id="affected-processor-families-and-models">Affected Processor Families and Models</h3>

<p>The following Intel processor families and models are affected by Intel-SA-00950:</p>

<ul>
  <li>
    <p>Intel Core processors, including Core i7, Core i5, Core i3, and Celeron (since IceLake)</p>
  </li>
  <li>
    <p>Intel Xeon processors (since Sephhire Rapids)</p>
  </li>
  <li>
    <p>Intel Atom processors</p>
  </li>
  <li>
    <p>Intel Pentium processors</p>
  </li>
</ul>

<h3 id="severity-classification-and-exploit-difficulty">Severity Classification and Exploit Difficulty</h3>

<p>The Intel-SA-00950 vulnerability is considered to be of high severity, with a CVSS score of 8.8 (High). This means that the vulnerability is considered to be very serious and could have a significant impact on affected systems.</p>

<p>However, it is true that exploiting it is considered to be moderately difficult due to the specific conditions that need to be met. An attacker would need to have access to the affected system and be able to run custom code. Additionally, the exploit would need to be carefully crafted to avoid detection by security measures.</p>

<p>Additionaly, as per the date of writing this post, the vulnerability is not believed to be actively exploited in the wild.</p>

<h2 id="amd-cachewarp-vulnerabilities-compromising-encrypted-virtual-machines">AMD Cachewarp Vulnerabilities: Compromising Encrypted Virtual Machines</h2>

<p>AMD Cachewarp vulnerability, tracked as CVE-2023-20592, is a software fault attack that targets AMD’s Secure Encrypted Virtualization (SEV) technology. SEV enhances the protection of Virtual Machines runing on AMD EPYC™ CPUs by using hardware-based memory encryption of the virtual machines’ memory, thus making exploits harder and providing protection against the hypervisor admin.</p>

<p>The CacheWarp vulnerability originates from a design flaw in the way SEV handles cache evictions. As a CPU processes data, it stores frequently accessed information in its cache, a high-speed memory that facilitates faster retrieval. When a cache line, a small unit of data, is no longer needed, it is evicted from the cache and marked as invalid. The eviction information is stored in a specific memory location.</p>

<p>The vulnerability allows an attacker to overwrite this eviction information with false data, which deceives the CPU into believing that the evicted cache line is still valid, causing it to reload the evicted data from memory, even if it has been modified by the attacker.</p>

<h3 id="leakage-and-control-the-perils-of-cachewarp">Leakage and Control: The Perils of CacheWarp</h3>

<p>The CacheWarp vulnerability can be exploited in two primary ways:</p>

<ul>
  <li>
    <p><strong>Leakage of Sensitive Information</strong>: By modifying the data in the reloaded cache line, the attacker can potentially gain access to sensitive information stored within the SEV-protected VM. This can have devastating consequences for organizations that rely on SEV to safeguard sensitive data.</p>
  </li>
  <li>
    <p><strong>Gaining Control of the VM</strong>: If the attacker manages to modify critical control data within the SEV-protected VM, they could gain full control of the VM, enabling them to execute arbitrary code and potentially disrupt operations.</p>
  </li>
</ul>

<h3 id="affected-processor-families-and-models-1">Affected Processor Families and Models</h3>

<p>The following AMD processor families and models are affected by CacheWarp:</p>

<ul>
  <li>
    <p>1st Gen AMD EPYC™ Processors (SEV and SEV-ES)</p>
  </li>
  <li>
    <p>2nd Gen AMD EPYC™ Processors (SEV and SEV-ES)</p>
  </li>
  <li>
    <p>3rd Gen AMD EPYC™ Processors (SEV, SEV-ES, SEV-SNP)</p>
  </li>
</ul>

<h3 id="severity-classification-and-exploit-difficulty-1">Severity Classification and Exploit Difficulty</h3>

<p>The CacheWarp vulnerability is considered to be of moderate severity, with a CVSS score of 5.3 (medium).</p>

<p>Exploiting it is considered to be relatively easy due to the fact that it only requires privileged access to the system. However, it is important to note that the exploit would need to be carefully crafted to avoid detection by security measures.</p>

<p>As per the date of writting this post, the vulnerability is not believed to be actively exploited in the wild.</p>

<h2 id="mitigation-and-scs-flavor-policy">Mitigation and SCS flavor policy</h2>

<p>Regarding the Intel-SA-00950 Linux distributors have published updated microcode called <code class="language-plaintext highlighter-rouge">intel-microcode</code> and updated kernels in the past weeks.</p>

<p>For the Ubuntu 22.04 LTS distribution normally used on SCS installations, the updates to the Intel microcode are described in <a href="https://ubuntu.com/security/notices/USN-6485-1">USN-6485-1</a>.</p>

<p>AMD has released software (microcode) patches to address the Cachewarp vulnerabilities in SEV-ES and SEV-SNP environments. However there is no mitigation available for first and second generations of EPYC™ processors, this is “Zen 1” (codename “Naples”) and “Zen 2” (codename “Rome”). The microcode patch is being deployed in two ways, as a standalone patch with an updated SEV firmware image and/or as part of a platform initialization (PI) package update. The updates on the microcode and firmware are described in <a href="https://www.amd.com/en/resources/product-security/bulletin/amd-sb-3005.html">AMD INVD Security Advisory</a>.</p>

<p>Regarding the Ubuntu 22.04 LTS distribution normally used on SCS installations, the updates to the AMD microcode were contained in the upstream commit <a href="https://git.kernel.org/pub/scm/linux/kernel/git/firmware/linux-firmware.git/commit/?id=b250b32ab1d044953af2dc5e790819a7703b7ee6">b250b32ab1d044953af2dc5e790819a7703b7ee6</a>, wich was provided in the package version <a href="https://launchpad.net/ubuntu/+source/amd64-microcode/3.20191218.1ubuntu2.2">amd64-microcode - 3.20191218.1ubuntu2.2</a>.</p>

<p>It should be noted that the encryption of VM memory from the AMD SEV extension is not used by default in SCS installations. We are not aware of it already being in use by any of the SCS cloud providers, so CacheWarp is likely not affecting any SCS provider currently. It is a feature designed specifically for high-security environments (trusted computing) and is expected to become more commonly used in the future though.</p>

<p>SCS requires providers of SCS-compatible IaaS to deploy fixes that are available upstream within a month of the availability.
This is mandated by the <a href="https://docs.scs.community/standards/scs-0100-v3-flavor-naming#complete-proposal-for-systematic-flavor-naming">SCS flavor naming standard</a>
– by not using the <code class="language-plaintext highlighter-rouge">i</code> (for insecure) suffix, they commit to keeping their compute hosts secured against such flaws by deploying the needed microcode, kernel and hypervisor fixes and mitigations within a month of their availability.</p>

<h2 id="conclusion">Conclusion</h2>
<p>Intel-SA-00950 and AMD Cachewarp vulnerabilities highlight the importance of vigilance and proactive cybersecurity measures. By promptly addressing these vulnerabilities and implementing preventative strategies, We can safeguard our systems, data, and users from potential security threats.</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://www.intel.com/content/www/us/en/security-center/advisory/intel-sa-00950.html">Intel-SA-00950 Security Advisory</a></li>
  <li><a href="https://www.intel.com/content/www/us/en/developer/topic-technology/software-security-guidance/processors-affected-consolidated-product-cpu-model.html">Intel-SA-00950 Affected Products</a></li>
  <li><a href="https://lock.cmpxchg8b.com/reptar.html">Reptar</a></li>
  <li><a href="https://cachewarpattack.com/">AMD CacheWarp</a></li>
  <li><a href="https://github.com/cispa/CacheWarp">AMD CacheWarp Proof-of-concept implementation</a></li>
</ul>]]></content><author><name>[&quot;David Rodríguez&quot;]</name></author><summary type="html"><![CDATA[The Evolving Landscape of Security Vulnerabilities]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://scs.community/blog/chip-scalpel.png" /><media:content medium="image" url="https://scs.community/blog/chip-scalpel.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">SCS enables - SCS is efficient to operate: Outcomes for R6</title><link href="https://scs.community/2023/12/29/scs-r6-enables/" rel="alternate" type="text/html" title="SCS enables - SCS is efficient to operate: Outcomes for R6" /><published>2023-12-29T00:00:00+00:00</published><updated>2024-03-21T11:48:02+00:00</updated><id>https://scs.community/2023/12/29/scs-r6-enables</id><content type="html" xml:base="https://scs.community/2023/12/29/scs-r6-enables/"><![CDATA[<h2 id="our-r6-outcomes">Our R6 outcomes</h2>

<p>To better align the development efforts within our community we started working with <em>outcomes</em>.
We began with this in the R4 cycle and even though we never published the outcomes for R5 - we <a href="https://github.com/SovereignCloudStack/website/pull/662">did have them for R5</a> as well ;)</p>

<p>These outcomes intend to outline what our development work aims to enable our users (Operators, Integrators and End-Users) to gain more value from SCS. This outlines our direction much better than talking about the next features that are planned and worked on. Furthermore, the outcomes assist us in our development work to ensure that every single epic and story we work on actually pays into the greater idea.</p>

<p>SCS delivers building blocks for digital sovereign cloud infrastructure.  We change the way infrastructure is operated and by doing this we want to enable operators to become more efficient and be able to scale better.</p>

<h4 id="working-with-outcomes-on-github">Working with outcomes on GitHub</h4>

<p>The outcomes allow us to look from various angles on our development efforts. Each outcome is reflected through a label within our GitHub repositories so that issues and pull requests can be labeled accordingly. Furthermore in the projects view we’ve added various filtered views to be able to quickly see which issues and working items play into which outcomes.</p>

<p>An <a href="https://github.com/orgs/SovereignCloudStack/projects/6/views/28">overall board</a> summarizes all of them.</p>

<h3 id="scs-is-standardized"><a href="https://github.com/orgs/SovereignCloudStack/projects/6/views/23">SCS is standardized</a></h3>

<p>While the S in SCS (at least the first ;)) stands for <em>Sovereign</em> it could also stand for <em>Standardized</em>.
SCS standardizes. Creating Standards within the community, reaching out to surrounding communities, and working with upstream as well as other players in our ecosystem to make sure we don’t “just reinvent the wheel” or stew in their own juice is a fair share of our work within the project.
The recent <a href="https://scs.community/2023/11/27/joint-standardization/">joint-efforts</a> with <a href="https://alasca.cloud">ALASCA</a> emphasize this even more.</p>

<p>Alongside the standardization work that has happened in the container and IaaS teams the <a href="https://scs.community/tenders/lot10">tender 10</a>, an important tender package for the standardization work, was kicked off at the beginning of the R5 cycle. Through the work in the SIG Standardization, a lot of the topics have gained pace in the last months, yet this is one of the major topics for the R6 cycle.</p>

<p>Two issues summarize the current efforts quite well:</p>

<ul>
  <li><a href="https://github.com/SovereignCloudStack/standards/issues/285">Missing IaaS Standards</a></li>
  <li><a href="https://github.com/sovereigncloudstack/issues/issues/181">SCS K8s cluster standardization</a></li>
</ul>

<h3 id="scs-is-understandable"><a href="https://github.com/orgs/SovereignCloudStack/projects/6/views/22">SCS is understandable</a></h3>

<p>In the last year the <a href="https://docs.scs.community">documentation page</a> came to life and is already very good in being the guide for the first steps into the SCS world. Due to the efforts of the SIG Documentation, it is not just a pile of documents but has a solid concept. However, for SCS to be understandable more than just awesome docs are needed. Collecting feedback from SCS integrators just as much as people and organizations that have their first touchpoint with SCS plays an important role in ensuring SCS is continuously becoming more understandable. Furthermore, deployment guides and architectural blueprints will be added.</p>

<p>The planned SCS lab environment will be built up following these documentation guidelines (this refers to the IaaS reference implementation). This will be a comprehensive examination of the validity and usability of this documentation.</p>

<h3 id="scs-enables"><a href="https://github.com/orgs/SovereignCloudStack/projects/6/views/20">SCS enables</a></h3>

<p>For the R4 development cycle one of the outcomes we proclaimed was <em>SCS enables Operators with an excellent toolbox</em>. An excellent toolbox is part of what is needed for operating cloud infrastructure as a commodity. Looking at what we’ve done in the past year this outcome however is much too narrow.
<em>SCS enables</em> - on a variety of levels and not just operators, but also integrators, developers, and most importantly consumers of cloud infrastructure built upon SCS who want to run on top of fully digital sovereign infrastructure.</p>

<p>With <a href="https://github.com/sovereignCloudStack/cluster-stacks">Cluster Stacks</a>, the V2 KaaS reference implementation, we provide an opinionated optimized configuration of Kubernetes clusters. Through better packaging, integrated testing, and bundled configuration SCS-based Kubernetes clusters can be individualized much easier.
Throughout the R6 development cycle Cluster Stacks are taken from a technical preview to be <a href="https://github.com/SovereignCloudStack/issues/milestone/8">functional and available on top of the IaaS reference implementation</a> as well. An overview to Cluster Stacks can be found <a href="https://scs.community/2023/12/23/clusterstacks/">in this blog post</a>.
The Cluster Stacks can already be tried out in a <a href="https://github.com/SovereignCloudStack/cluster-stacks-demo">demo</a>. Although this is based on the non production ready provider Docker, the usage is the same for every provider.</p>

<p>Early in the R6 development cycle, the <a href="https://scs.community/tenders/lot4">software defined networking tender VP04</a> was kicked of. As part of this work, we want to make sure that
networking not only scales superior in the IaaS reference implementation but also enables inter-cloud connectivity between
workloads on all layers.</p>

<p>Through GitHub, the <a href="https://github.com/SovereignCloudStack/issues/issues?q=is%3Aopen+is%3Aissue+label%3ASCS-VP04">list of network-related epics and user stories</a> that are part of VP04 can be viewed.</p>

<p>With our work on the <a href="https://github.com/SovereignCloudStack/issues/issues/184">domain manager role</a> we’re addressing a topic that has been a hurdle for public clouds and self-management by customers. Lots of public clouds, built on top of OpenStack, have developed their own ways to work around the missing possibility of domain management.</p>

<h3 id="scs-is-transparent"><a href="https://github.com/orgs/SovereignCloudStack/projects/6/views/30">SCS is transparent</a></h3>

<p>Transparency is one of the core values embedded in our project - yet we want to make sure our development efforts throughout the R6 cycle are actively working towards being transparent. This ranges from our development culture (which needs to be transparent not only to be trustworthy but also to lower the barrier to join the community) to the open operations movement, the future of the SCS project, and all the way to technical items such as <a href="https://en.wikipedia.org/wiki/Software_supply_chain">SBOMs</a> for our complete stack.
Transparent projects can be audited easily and trust can be built up more easily.</p>

<p>Another important factor that plays into transparency is being transparent about security aspects, and incidents and proactively pursuing providing a secure reference implementation.</p>

<p>For our status page initiative, an initial MVP was done in the R5 development cycle. In the R6 development cycle we <a href="https://github.com/SovereignCloudStack/issues/issues?q=is%3Aopen+is%3Aissue+label%3Astatus-page">finalize</a> what we’ve evaluated with the MVP and are going to ship a modern status page application. The release of the status page application (which itself is divided into <a href="https://github.com/SovereignCloudStack/status-page-web">frontend</a>, <a href="https://github.com/sovereignCloudStack/status-page-api">backend</a> as well as the <a href="https://github.com/SovereignCloudStack/status-page-openapi">OpenAPI spec</a> and a <a href="https://github.com/SovereignCloudStack/status-page-deployment">repository holding the deployment logic</a>) is de-coupled from the SCS Releases.</p>

<h3 id="scs-is-continuously-built-and-tested"><a href="https://github.com/orgs/SovereignCloudStack/projects/6/views/21">SCS is continuously built and tested</a></h3>

<p>With <a href="https://zuul.scs.community">our Zuul</a> in place (thanks to the efforts throughout the R5 development cycle) it is now time to shift a lot of our test runs from GitHub actions to the Zuul instance.
As part of the <a href="https://scs.community/tenders/lot1">tender VP01</a> the test coverage of the foundations the IaaS reference implementation builds upon is continuously being extended.</p>

<h3 id="scs-is-opinionated"><a href="https://github.com/orgs/SovereignCloudStack/projects/6/views/29">SCS is opinionated</a></h3>

<p>While the SCS projects provide a modular stack and strongly work towards interoperability, we want to be opinionated in our reference implementation. Being opinionated on that level leads to focus and avoids having too many loose ends. Good examples of this is how we address verticals such as our IAM or observability stacks.</p>

<h3 id="scs-charters-new-territory">SCS charters new territory</h3>

<p>While a lot of our activities involve working upstream with the various communities doing our part of ensuring the components that SCS is built upon are staying healthy, we do want to push the boundaries of what is possible further and see where we can find ways to provide better cloud infrastructure. This does mean to sail into unchartered waters - sometimes having to turn around, hit rock bottom or actually find a new cool passage.
Endeavors like the evaluation of <a href="https://github.com/SovereignCloudStack/issues/issues?q=is%3Aopen+is%3Aissue+label%3ASCS-VP04+sonic">building community SONiC</a> images, revisiting the topic of cloud observability and testing and engaging into the discussion about the discoverability of flavors and functionality of OpenStack clouds.</p>]]></content><author><name>[&quot;Felix Kronlage-Dammers&quot;]</name></author><summary type="html"><![CDATA[Our R6 outcomes]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://scs.community/default-card.jpg" /><media:content medium="image" url="https://scs.community/default-card.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Cluster Stacks</title><link href="https://scs.community/2023/12/23/clusterstacks/" rel="alternate" type="text/html" title="Cluster Stacks" /><published>2023-12-23T00:00:00+00:00</published><updated>2024-03-21T11:48:02+00:00</updated><id>https://scs.community/2023/12/23/clusterstacks</id><content type="html" xml:base="https://scs.community/2023/12/23/clusterstacks/"><![CDATA[<h2 id="cluster-stacks">Cluster Stacks</h2>

<p>With Release 5, the Sovereign Cloud Stack has published a technical preview of the “Cluster Stacks” - a framework that simplifies the productive operation of Kubernetes clusters from the user’s perspective and offers transparency regarding the configuration and composition of the cluster’s core components.
SCS is thus creating a decentralized and user-oriented alternative to the often non-transparent and non cross-compatible commercial “Managed Kubernetes” offerings.</p>

<h3 id="kubernetes---more-difficult-than-it-seems">Kubernetes - More difficult than it seems</h3>

<p>Operating a Kubernetes cluster is in principle not difficult. However, simple Kubernetes clusters generally do not meet the requirements for production operation in terms of stability, security and reliability. A Kubernetes cluster in a production environment requires extensive configuration with regard to important core applications and node images to be, for example, to better protected against security incidents. In practice, a lack of knowledge often has the consequence that companies use unsecured clusters for real workloads, which results in a significant security risk.
Another frequent problem is the missing test infrastructure, without which updates to production clusters are prone to errors. As a result, clusters are updated only once a year as forced by a no longer supported Kubernetes version.</p>

<h3 id="proprietary-and-intransparent-solutions">Proprietary and intransparent Solutions</h3>

<p>Many users therefore either rely on a “Managed Kubernetes” service from an infrastructure provider or, if they operate the Kubernetes environment themselves, on a commercial and therefore usually proprietary product (e.g. OpenShift).
These solutions are usually so specific in terms of configuration and component selection that changing them is extremely time-consuming and resource-intensive. In addition, a hosted “managed” solution is often opaque with respect to structure and function - which creates an additional dependency. It is rarely clear how clusters are structured and which steps a customer still has to carry out independently in order to operate a cluster securely and ready for production. As these steps are different for each provider, migrations or the use of multiple providers are correspondingly difficult.
Depending on the service, the user may have more or less responsibility. This can mean that certain security settings or important components are already built into one service but lacking in another one. Every time a user switches providers, they must therefore take a close look at the details of the product so that they can draw the right conclusions and implement the missing settings and install the missing components themselves.</p>

<h3 id="cluster-stacks---the-solution-for-production-ready-and-simple-kubernetes">Cluster Stacks - The solution for production-ready and simple Kubernetes</h3>

<p>This is where the cluster stacks from SCS come in and create a solution in which the structure of a cluster is transparent or completely open-source, even with a “managed” solution, and users can join together in a community to improve the managed solution together with the providers. Like the V1 of the SCS KaaS reference implementation it is based on the open-source Kubernetes project “Cluster API”, which makes it possible to create and operate secure clusters quickly and easily - even on different providers.
Cluster stacks are a concept that combines all the important components of a Kubernetes cluster. The three main components are:</p>
<ol>
  <li>Configuration of Kubernetes (e.g. Kubeadm),</li>
  <li>Core Applications,</li>
  <li>Node Images
These three elements are combined in cluster stacks and tested as a whole. a cluster stack is released only if everything works together and an upgrade from the previous version is possible and its function thus is ensured. This avoids the frequent problems that occur when updating Kubernetes clusters.
In addition, the <a href="https://github.com/SovereignCloudStack//cluster-stack-operator">SCS Cluster Stack Operator</a> simplifies the use of <a href="https://github.com/SovereignCloudStack//cluster-stacks">Cluster Stacks</a>. Together with the cluster API, simple API calls can be used to create new, production-ready clusters or update existing ones.</li>
</ol>

<h3 id="cluster-stacks-as-framework">Cluster Stacks as Framework</h3>

<p>The cluster stacks are a concept that is open to all providers and allows that clusters can be created and managed on the basis of the cluster API. Existing cluster stacks can be further utilized or individually created, e.g. to support additional providers or add specific requirements.
As part of the SCS project, cluster stacks are configured for selected providers and therefore guarantee SCS-standardized use of Kubernetes. Apart from the fact that Cluster API is a prerequisite technology, the cluster stacks have no dependencies.</p>

<h3 id="outlook">Outlook</h3>

<p>The R5 release contains a technical preview of the cluster stacks and the associated operator, as well as a <a href="https://github.com/SovereignCloudStack/cluster-stacks-demo">demo</a>, which allows users to start clusters locally using Docker.
An OpenStack interface for OpenStack will be released in the coming weeks so that the cluster stacks can be used on “real” cloud infrastructure.
For the release R6 a migration path taking existing installations from the V1 SCS KaaS reference implementation to Cluster Stacks is planned.</p>]]></content><author><name>[&quot;Janis Kemper&quot;, &quot;Sven Batista Steinbach&quot;, &quot;Jan Schoone&quot;, &quot;Alexander Diab&quot;]</name></author><summary type="html"><![CDATA[Cluster Stacks]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://scs.community/default-card.jpg" /><media:content medium="image" url="https://scs.community/default-card.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">SCS observability and monitoring - An opinionated proposal</title><link href="https://scs.community/tech/2023/12/06/mvp-monitoring/" rel="alternate" type="text/html" title="SCS observability and monitoring - An opinionated proposal" /><published>2023-12-06T00:00:00+00:00</published><updated>2024-03-21T11:48:02+00:00</updated><id>https://scs.community/tech/2023/12/06/mvp-monitoring</id><content type="html" xml:base="https://scs.community/tech/2023/12/06/mvp-monitoring/"><![CDATA[<p>We’re excited to announce that our Minimum Viable Product (MVP) for extending the monitoring from the IaaS layer to a solution that covers observability and monitoring full-stack has achieved a presentable and viable state. MVP aims to provide a global
monitoring view of various SCS layers and components, including KaaS, IaaS, and actively
used services within the SCS community such as Jitsi, HedgeDoc, Nextcloud, Harbor container
registry, and more. The global view is made possible by the Observer monitoring solution
we’ve designed. The Observer enables the collection, storage, and visualization of metrics
from the SCS infrastructure.</p>

<p>While the Observer monitoring solution is currently part of the MVP, our vision is to evolve
it into an SCS product in future versions, once it reaches the required stability. Monitoring
of the KaaS layer, monitoring of the IaaS layer, and monitoring of infrastructure services
are illustrative examples of how this Observer monitoring solution can be utilized. Furthermore,
the stack employed for observing the KaaS layer and infrastructure services is also designed to
evolve into an SCS product in future stable versions.</p>

<h2 id="the-architecture">The architecture</h2>

<p>The diagram below provides a high-level overview of the architecture implemented in our MVP.
To explore the code and detailed documentation, head over to the <a href="https://github.com/sovereignCloudStack/k8s-observability">k8s-observability</a> repository.
If you’re keen on delving deeper, check out the repository’s README and try setting up your
own MVP deployment locally using KinD.</p>

<p>Furthermore, we’ve deployed the MVP publicly, offering handy and well-organized dashboards
that showcase metrics gathered from the SCS infrastructure. Be sure to check out the upcoming
blog section for the Observer (Grafana UI) links. It’s a must-see!</p>

<p>To enhance your experience, we’ve made the monitoring of the KaaS layer more interactive.
You can create your own Kubernetes cluster and observe how it gets registered and
visualized in the Observer. <a href="#monitoring-of-the-kaas-layer">It’s worth giving it a try!</a></p>

<figure class="figure mx-auto d-block" style="width:50%">
  <a href="/assets/images/blog/mvp_monitoring_architecture-943664a3a18ae79aa5d3aed3edc0f63cc198de734497f4246c51b3bdf81978e7ced339bc2b40ac13310ad41004264333ab43d80e87eb9b693b96f80ee43da541.png">
    <img class="figure-img w-100" integrity="sha512-lDZko6GK55ql067T7cD2PMGY3nNEl/QkbFGzvfgZeOfO0zm8K0CsEzEK1BAEJkMzq0PYDofrm2k7lvgO5D2lQQ==" crossorigin="anonymous" src="/assets/images/blog/mvp_monitoring_architecture-943664a3a18ae79aa5d3aed3edc0f63cc198de734497f4246c51b3bdf81978e7ced339bc2b40ac13310ad41004264333ab43d80e87eb9b693b96f80ee43da541.png" />
  </a>
</figure>

<p>The SCS Monitoring MVP is developed on the foundation of the <a href="https://github.com/dNationCloud/kubernetes-monitoring">dNation monitoring solution</a>.</p>

<p><em>Disclaimer: Please be aware that the MVP demo URLs referenced in the sections below are
expected to be unavailable within the next couple of months. We appreciate your understanding.</em></p>

<h2 id="monitoring-of-the-kaas-layer">Monitoring of the KaaS Layer</h2>

<p>As mentioned earlier, we aim to showcase how the Observer monitors the CSP KaaS offering.
To make this demonstration interactive, we’ve developed a KaaS mock service, allowing
you to easily create a Kubernetes cluster.</p>

<p>The KaaS mock service is accessible through <a href="http://213.131.230.7:8080/kaas">Swagger UI</a>.</p>

<p>Don’t hesitate to <a href="http://213.131.230.7:8080/kaas#/Clusters/create_cluster_api_clusters__post">create</a> your own Kubernetes cluster
within approximately three minutes. You’ll find it displayed on the dedicated <a href="http://213.131.230.73:30000/d/kaas-monitoring/kaas-monitoring">Observer high-level dashboard</a>.
For a deeper exploration of metrics, simply drill down and click on the cluster you created.
Feel free to experiment, such as triggering monitoring <a href="http://213.131.230.73:30001/#/alerts">alerts</a> by initiating actions like destroying specific components 😎.</p>

<p>Please note that there is a restriction of a maximum of 10 clusters, as the KaaS mock
service operates on a single VM. Additionally, it’s important to keep in mind that this
monitoring solution isn’t real-time, meaning it doesn’t react immediately to your actions.
However, you can typically expect a response within a few minutes.</p>

<h2 id="monitoring-of-the-iaas-layer">Monitoring of the IaaS Layer</h2>

<p>To highlight the Observer’s capability in monitoring the IaaS layer, we utilized the <a href="https://docs.scs.community/docs/iaas/guides/deploy-guide/examples/testbed">OSISM testbed</a>
deployment as a demonstration within this MVP. The OSISM testbed has been patched,
enabling the Observer to scrape metrics via the Thanos sidecar.
Explore the dedicated <a href="http://213.131.230.73:30000/d/testbed/iaas-monitoring">Observer high-level dashboard</a> for a list of alerts
present in our OSISM testbed and access links to dashboards we copied (and made functional) from the testbed Grafana.</p>

<h2 id="monitoring-of-infrastructure-services">Monitoring of Infrastructure Services</h2>

<p>To demonstrate the Observer’s capability in monitoring CSP infrastructure services deployed
on top of the IaaS layer, we chose to monitor key SCS infrastructure elements such as the
SCS Observer cluster itself, the SCS Harbor Kubernetes cluster, and various endpoints
of SCS services regularly used by the SCS community, including the SCS webpage, SCS docs page, SCS Jitsi instance,
SCS HedgeDoc, and more.
Explore the <a href="http://213.131.230.73:30000/d/monitoring/infrastructure-services-monitoring">high-level infrastructure dashboard</a>, drill down,
and click on the service you want to investigate deeper.</p>

<p>For example, dive into detailed Harbor metrics by following this path: <a href="http://213.131.230.73:30000/d/monitoring/infrastructure-services-monitoring">infrastructure dashboard</a> -&gt; <a href="http://213.131.230.73:30000/d/e1b111ecbb5185e637d5a7eef26e850f/kubernetes-monitoring-harbor?refresh=10s&amp;var-datasource=PC96415006F908B67&amp;var-cluster=harbor-cluster">harbor cluster</a> -&gt; scroll to the bottom and click on the <a href="http://213.131.230.73:30000/d/harbor/harbor?var-job=harbor&amp;refresh=10s&amp;var-datasource=PC96415006F908B67&amp;var-cluster=harbor-cluster">harbor app dashboard</a>.</p>

<p>🔍 Happy monitoring!</p>]]></content><author><name>[&quot;Matej Feder&quot;]</name></author><category term="tech" /><summary type="html"><![CDATA[We’re excited to announce that our Minimum Viable Product (MVP) for extending the monitoring from the IaaS layer to a solution that covers observability and monitoring full-stack has achieved a presentable and viable state. MVP aims to provide a global monitoring view of various SCS layers and components, including KaaS, IaaS, and actively used services within the SCS community such as Jitsi, HedgeDoc, Nextcloud, Harbor container registry, and more. The global view is made possible by the Observer monitoring solution we’ve designed. The Observer enables the collection, storage, and visualization of metrics from the SCS infrastructure.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://scs.community/default-card.jpg" /><media:content medium="image" url="https://scs.community/default-card.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>